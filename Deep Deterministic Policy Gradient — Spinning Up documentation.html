<!DOCTYPE html>
<!-- saved from url=(0060)https://spinningup.openai.com/en/latest/algorithms/ddpg.html -->
<html class=" js flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers no-applicationcache svg inlinesvg smil svgclippaths" lang="en" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deep Deterministic Policy Gradient — Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="https://spinningup.openai.com/en/latest/_static/openai_icon.ico">
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/theme.css" type="text/css">
  <link rel="stylesheet" href="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/pygments.css" type="text/css">
  <link rel="stylesheet" href="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/modify.css" type="text/css">
    <link rel="index" title="Index" href="https://spinningup.openai.com/en/latest/genindex.html">
    <link rel="search" title="Search" href="https://spinningup.openai.com/en/latest/search.html">
    <link rel="next" title="Twin Delayed DDPG" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">
    <link rel="prev" title="Proximal Policy Optimization" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html"> 

  
  <script type="text/javascript" async="" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/analytics.js.Download"></script><script type="text/javascript" async="" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/js"></script><script type="text/javascript" async="" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/analytics.js.Download"></script><script src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/modernizr.min.js.Download"></script>


<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">

<link rel="stylesheet" href="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/readthedocs-doc-embed.css" type="text/css">

<script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/readthedocs-data.js.Download"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = "algorithms/ddpg"
READTHEDOCS_DATA['source_suffix'] = ".rst"
</script>

<script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/readthedocs-analytics.js.Download"></script><script src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/js(1)" type="text/javascript" async=""></script>

<!-- end RTD <extrahead> -->
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="https://spinningup.openai.com/en/latest/index.html">
          

          
            
            <img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/spinning-up-logo2.png" class="logo" alt="Logo">
          
          </a>

          
            
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="https://spinningup.openai.com/en/latest/search.html" method="get">
    <input type="text" name="q" placeholder="Search docs">
    <input type="hidden" name="check_keywords" value="yes">
    <input type="hidden" name="area" value="default">
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/user/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/user/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/user/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/user/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction to RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">Part 1: Key Concepts in RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">Part 2: Kinds of RL Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html">Part 3: Intro to Policy Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">Key Papers in Deep RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/exercises.html">Exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/spinningup/bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms Docs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1 current"><a class="reference internal current" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#"><span class="toctree-expand"></span>Deep Deterministic Policy Gradient</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background"><span class="toctree-expand"></span>Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#quick-facts">Quick Facts</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#key-equations"><span class="toctree-expand"></span>Key Equations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-q-learning-side-of-ddpg">The Q-Learning Side of DDPG</a></li>
<li class="toctree-l4"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-policy-learning-side-of-ddpg">The Policy Learning Side of DDPG</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#exploration-vs-exploitation">Exploration vs. Exploitation</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode">Pseudocode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation"><span class="toctree-expand"></span>Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation-pytorch-version">Documentation: PyTorch Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#saved-model-contents-pytorch-version">Saved Model Contents: PyTorch Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation-tensorflow-version">Documentation: Tensorflow Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#saved-model-contents-tensorflow-version">Saved Model Contents: Tensorflow Version</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#references"><span class="toctree-expand"></span>References</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#relevant-papers">Relevant Papers</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#why-these-papers">Why These Papers?</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#other-public-implementations">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://spinningup.openai.com/en/latest/etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="https://spinningup.openai.com/en/latest/index.html">Docs</a> »</li>
        
      <li>Deep Deterministic Policy Gradient</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/openai/spinningup/blob/master/docs/algorithms/ddpg.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deep-deterministic-policy-gradient">
<h1><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id1">Deep Deterministic Policy Gradient</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#deep-deterministic-policy-gradient" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#deep-deterministic-policy-gradient" id="id1">Deep Deterministic Policy Gradient</a><ul>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background" id="id2">Background</a><ul>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#quick-facts" id="id3">Quick Facts</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#key-equations" id="id4">Key Equations</a><ul>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-q-learning-side-of-ddpg" id="id5">The Q-Learning Side of DDPG</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-policy-learning-side-of-ddpg" id="id6">The Policy Learning Side of DDPG</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#exploration-vs-exploitation" id="id7">Exploration vs. Exploitation</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode" id="id8">Pseudocode</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation" id="id9">Documentation</a><ul>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation-pytorch-version" id="id10">Documentation: PyTorch Version</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#saved-model-contents-pytorch-version" id="id11">Saved Model Contents: PyTorch Version</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation-tensorflow-version" id="id12">Documentation: Tensorflow Version</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#saved-model-contents-tensorflow-version" id="id13">Saved Model Contents: Tensorflow Version</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#references" id="id14">References</a><ul>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#relevant-papers" id="id15">Relevant Papers</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#why-these-papers" id="id16">Why These Papers?</a></li>
<li><a class="reference internal" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#other-public-implementations" id="id17">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<h2><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id2">Background</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background" title="Permalink to this headline">¶</a></h2>
<p>(Previously: <a class="reference external" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-optimal-q-function-and-the-optimal-action">Introduction to RL Part 1: The Optimal Q-Function and the Optimal Action</a>)</p>
<p>Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.</p>
<p>This approach is closely connected to Q-learning, and is motivated the same way: if you know the optimal action-value function <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg" alt="Q^*(s,a)">, then in any given state, the optimal action <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/baf715aa6a295b7b7d85e1e1123552c5ae705756.svg" alt="a^*(s)"> can be found by solving</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/82f049ec26e21eb2bfc6af21e3465707814f4838.svg" alt="a^*(s) = \arg \max_a Q^*(s,a)."></p>
</div><p>DDPG interleaves learning an approximator to <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg" alt="Q^*(s,a)"> with learning an approximator to <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/baf715aa6a295b7b7d85e1e1123552c5ae705756.svg" alt="a^*(s)">, and it does so in a way which is specifically adapted for environments with continuous action spaces. But what does it mean that DDPG is adapted <em>specifically</em> for environments with continuous action spaces? It relates to how we compute the max over actions in <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/1f3098d0653722949f8ceeefc8b5c951d99c8274.svg" alt="\max_a Q^*(s,a)">.</p>
<p>When there are a finite number of discrete actions, the max poses no problem, because we can just compute the Q-values for each action separately and directly compare them. (This also immediately gives us the action which maximizes the Q-value.) But when the action space is continuous, we can’t exhaustively evaluate the space, and solving the optimization problem is highly non-trivial. Using a normal optimization algorithm would make calculating <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/1f3098d0653722949f8ceeefc8b5c951d99c8274.svg" alt="\max_a Q^*(s,a)"> a painfully expensive subroutine. And since it would need to be run every time the agent wants to take an action in the environment, this is unacceptable.</p>
<p>Because the action space is continuous, the function <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg" alt="Q^*(s,a)"> is presumed to be differentiable with respect to the action argument. This allows us to set up an efficient, gradient-based learning rule for a policy <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/3c89236fa57c3dbe71f7c249a07267f83d9c638b.svg" alt="\mu(s)"> which exploits that fact. Then, instead of running an expensive optimization subroutine each time we wish to compute <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/03f01f77446d623f1c933e335f9f81c9a3558c4f.svg" alt="\max_a Q(s,a)">, we can approximate it with <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/8070b852fa94029e80d5811417fd76818a31ec4c.svg" alt="\max_a Q(s,a) \approx Q(s,\mu(s))">. See the Key Equations section details.</p>
<div class="section" id="quick-facts">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id3">Quick Facts</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#quick-facts" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>DDPG is an off-policy algorithm.</li>
<li>DDPG can only be used for environments with continuous action spaces.</li>
<li>DDPG can be thought of as being deep Q-learning for continuous action spaces.</li>
<li>The Spinning Up implementation of DDPG does not support parallelization.</li>
</ul>
</div>
<div class="section" id="key-equations">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id4">Key Equations</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#key-equations" title="Permalink to this headline">¶</a></h3>
<p>Here, we’ll explain the math behind the two parts of DDPG: learning a Q function, and learning a policy.</p>
<div class="section" id="the-q-learning-side-of-ddpg">
<h4><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id5">The Q-Learning Side of DDPG</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-q-learning-side-of-ddpg" title="Permalink to this headline">¶</a></h4>
<p>First, let’s recap the Bellman equation describing the optimal action-value function, <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg" alt="Q^*(s,a)">. It’s given by</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/3a8b6ce0d6c0b68744b5724403f5d70ed5cda5db.svg" alt="Q^*(s,a) = \underset{s&#39; \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a&#39;} Q^*(s&#39;, a&#39;)\right]"></p>
</div><p>where <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/411171ab57c4bec0d86c9f4b495106ba5d73decc.svg" alt="s&#39; \sim P"> is shorthand for saying that the next state, <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg" alt="s&#39;">, is sampled by the environment from a distribution <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/400976c62fa52ed70c85d7389f039b5e41473654.svg" alt="P(\cdot| s,a)">.</p>
<p>This Bellman equation is the starting point for learning an approximator to <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg" alt="Q^*(s,a)">. Suppose the approximator is a neural network <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/521198ffdba43bf32186f95801549cd1502b76c7.svg" alt="Q_{\phi}(s,a)">, with parameters <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/3b22abcadf8773922f8db80011611bad8123a783.svg" alt="\phi">, and that we have collected a set <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/452456a08130b84d0c030fdc6e9b05973c5bc8b2.svg" alt="{\mathcal D}"> of transitions <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/4d273c4abe9c8d2805d78e826ee4368ed92841d7.svg" alt="(s,a,r,s&#39;,d)"> (where <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/9d61e89bfc1aa6993172a3ac47ab5be75f8e9e81.svg" alt="d"> indicates whether state <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg" alt="s&#39;"> is terminal). We can set up a <strong>mean-squared Bellman error (MSBE)</strong> function, which tells us roughly how closely <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/c25464faf1bf4928960905461cbbabe1d2441cb2.svg" alt="Q_{\phi}"> comes to satisfying the Bellman equation:</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/31dda6ac0678255c4e192dd6fae4f7ed3c7cd91b.svg" alt="L(\phi, {\mathcal D}) = \underset{(s,a,r,s&#39;,d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a&#39;} Q_{\phi}(s&#39;,a&#39;) \right) \Bigg)^2
    \right]"></p>
</div><p>Here, in evaluating <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/4591928b993b71d80f43193ffbbbef8e9f3aea10.svg" alt="(1-d)">, we’ve used a Python convention of evaluating <code class="docutils literal"><span class="pre">True</span></code> to 1 and <code class="docutils literal"><span class="pre">False</span></code> to zero. Thus, when <code class="docutils literal"><span class="pre">d==True</span></code>—which is to say, when <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/6e85fa05d4954e7c1e8037ee1bd163d15bc2e2d6.svg" alt="s&#39;"> is a terminal state—the Q-function should show that the agent gets no additional rewards after the current state. (This choice of notation corresponds to what we later implement in code.)</p>
<p>Q-learning algorithms for function approximators, such as DQN (and all its variants) and DDPG, are largely based on minimizing this MSBE loss function. There are two main tricks employed by all of them which are worth describing, and then a specific detail for DDPG.</p>
<p><strong>Trick One: Replay Buffers.</strong> All standard algorithms for training a deep neural network to approximate <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg" alt="Q^*(s,a)"> make use of an experience replay buffer. This is the set <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/452456a08130b84d0c030fdc6e9b05973c5bc8b2.svg" alt="{\mathcal D}"> of previous experiences. In order for the algorithm to have stable behavior, the replay buffer should be large enough to contain a wide range of experiences, but it may not always be good to keep everything. If you only use the very-most recent data, you will overfit to that and things will break; if you use too much experience, you may slow down your learning. This may take some tuning to get right.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">We’ve mentioned that DDPG is an off-policy algorithm: this is as good a point as any to highlight why and how. Observe that the replay buffer <em>should</em> contain old experiences, even though they might have been obtained using an outdated policy. Why are we able to use these at all? The reason is that the Bellman equation <em>doesn’t care</em> which transition tuples are used, or how the actions were selected, or what happens after a given transition, because the optimal Q-function should satisfy the Bellman equation for <em>all</em> possible transitions. So any transitions that we’ve ever experienced are fair game when trying to fit a Q-function approximator via MSBE minimization.</p>
</div>
<p><strong>Trick Two: Target Networks.</strong> Q-learning algorithms make use of <strong>target networks</strong>. The term</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/fac308175faa67be9f5b27260abaf0ae6c4a58bb.svg" alt="r + \gamma (1 - d) \max_{a&#39;} Q_{\phi}(s&#39;,a&#39;)"></p>
</div><p>is called the <strong>target</strong>, because when we minimize the MSBE loss, we are trying to make the Q-function be more like this target. Problematically, the target depends on the same parameters we are trying to train: <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/3b22abcadf8773922f8db80011611bad8123a783.svg" alt="\phi">. This makes MSBE minimization unstable. The solution is to use a set of parameters which comes close to <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/3b22abcadf8773922f8db80011611bad8123a783.svg" alt="\phi">, but with a time delay—that is to say, a second network, called the target network, which lags the first. The parameters of the target network are denoted <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/3d9fb7e74f48ade89cbbcc0f3d1f3cb89a824864.svg" alt="\phi_{\text{targ}}">.</p>
<p>In DQN-based algorithms, the target network is just copied over from the main network every some-fixed-number of steps. In DDPG-style algorithms, the target network is updated once per main network update by polyak averaging:</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/d417987803ca9f61ac60741880a748129bd66dde.svg" alt="\phi_{\text{targ}} \leftarrow \rho \phi_{\text{targ}} + (1 - \rho) \phi,"></p>
</div><p>where <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg" alt="\rho"> is a hyperparameter between 0 and 1 (usually close to 1). (This hyperparameter is called <code class="docutils literal"><span class="pre">polyak</span></code> in our code).</p>
<p><strong>DDPG Detail: Calculating the Max Over Actions in the Target.</strong> As mentioned earlier: computing the maximum over actions in the target is a challenge in continuous action spaces. DDPG deals with this by using a <strong>target policy network</strong> to compute an action which approximately maximizes <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/a50d5d2b71fa30f115adf18b0bb1354f967b064a.svg" alt="Q_{\phi_{\text{targ}}}">. The target policy network is found the same way as the target Q-function: by polyak averaging the policy parameters over the course of training.</p>
<p>Putting it all together, Q-learning in DDPG is performed by minimizing the following MSBE loss with stochastic gradient descent:</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/4421120861d55302d76c7e2fd7cc5b2da7aea320.svg" alt="L(\phi, {\mathcal D}) = \underset{(s,a,r,s&#39;,d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) Q_{\phi_{\text{targ}}}(s&#39;, \mu_{\theta_{\text{targ}}}(s&#39;)) \right) \Bigg)^2
    \right],"></p>
</div><p>where <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/a325c9e05fa2ccce85eb2384ca00b4888d1c7824.svg" alt="\mu_{\theta_{\text{targ}}}"> is the target policy.</p>
</div>
<div class="section" id="the-policy-learning-side-of-ddpg">
<h4><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id6">The Policy Learning Side of DDPG</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-policy-learning-side-of-ddpg" title="Permalink to this headline">¶</a></h4>
<p>Policy learning in DDPG is fairly simple. We want to learn a deterministic policy <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/6923cb2043e84ea05d3eddbb7436c60659243cb9.svg" alt="\mu_{\theta}(s)"> which gives the action that maximizes <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/521198ffdba43bf32186f95801549cd1502b76c7.svg" alt="Q_{\phi}(s,a)">. Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/cc4e3565d839e63e871a1cf7e3ce5e95bb616b29.svg" alt="\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi}(s, \mu_{\theta}(s)) \right]."></p>
</div><p>Note that the Q-function parameters are treated as constants here.</p>
</div>
</div>
<div class="section" id="exploration-vs-exploitation">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id7">Exploration vs. Exploitation</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#exploration-vs-exploitation" title="Permalink to this headline">¶</a></h3>
<p>DDPG trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals. To make DDPG policies explore better, we add noise to their actions at training time. The authors of the original DDPG paper recommended time-correlated <a class="reference external" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process">OU noise</a>, but more recent results suggest that uncorrelated, mean-zero Gaussian noise works perfectly well. Since the latter is simpler, it is preferred. To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.)</p>
<p>At test time, to see how well the policy exploits what it has learned, we do not add noise to the actions.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">Our DDPG implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the <code class="docutils literal"><span class="pre">start_steps</span></code> keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration.</p>
</div>
</div>
<div class="section" id="pseudocode">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id8">Pseudocode</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/5811066e89799e65be299ec407846103fcf1f746.svg" alt="\begin{algorithm}[H]
    \caption{Deep Deterministic Policy Gradient}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ}} \leftarrow \phi$
    \REPEAT
        \STATE Observe state $s$ and select action $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{Low}, a_{High})$, where $\epsilon \sim \mathcal{N}$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s&#39;$, reward $r$, and done signal $d$ to indicate whether $s&#39;$ is terminal
        \STATE Store $(s,a,r,s&#39;,d)$ in replay buffer $\mathcal{D}$
        \STATE If $s&#39;$ is terminal, reset environment state.
        \IF{it&#39;s time to update}
            \FOR{however many updates}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s&#39;,d) \}$ from $\mathcal{D}$
                \STATE Compute targets
                \begin{equation*}
                    y(r,s&#39;,d) = r + \gamma (1-d) Q_{\phi_{\text{targ}}}(s&#39;, \mu_{\theta_{\text{targ}}}(s&#39;))
                \end{equation*}
                \STATE Update Q-function by one step of gradient descent using
                \begin{equation*}
                    \nabla_{\phi} \frac{1}{|B|}\sum_{(s,a,r,s&#39;,d) \in B} \left( Q_{\phi}(s,a) - y(r,s&#39;,d) \right)^2
                \end{equation*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi}(s, \mu_{\theta}(s))
                \end{equation*}
                \STATE Update target networks with
                \begin{align*}
                    \phi_{\text{targ}} &amp;\leftarrow \rho \phi_{\text{targ}} + (1-\rho) \phi \\
                    \theta_{\text{targ}} &amp;\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}"></p>
</div></div>
</div>
<div class="section" id="documentation">
<h2><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id9">Documentation</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation" title="Permalink to this headline">¶</a></h2>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">In what follows, we give documentation for the PyTorch and Tensorflow implementations of DDPG in Spinning Up. They have nearly identical function calls and docstrings, except for details relating to model construction. However, we include both full docstrings for completeness.</p>
</div>
<div class="section" id="documentation-pytorch-version">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id10">Documentation: PyTorch Version</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation-pytorch-version" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="spinup.ddpg_pytorch">
<code class="descclassname">spinup.</code><code class="descname">ddpg_pytorch</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;MagicMock spec='str' id='140554320181456'&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=100</em>, <em>replay_size=1000000</em>, <em>gamma=0.99</em>, <em>polyak=0.995</em>, <em>pi_lr=0.001</em>, <em>q_lr=0.001</em>, <em>batch_size=100</em>, <em>start_steps=10000</em>, <em>update_after=1000</em>, <em>update_every=50</em>, <em>act_noise=0.1</em>, <em>num_test_episodes=10</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=1</em><span class="sig-paren">)</span><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#spinup.ddpg_pytorch" title="Permalink to this definition">¶</a></dt>
<dd><p>Deep Deterministic Policy Gradient (DDPG)</p>
<table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name">
<col class="field-body">
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> – A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> – <p>The constructor method for a PyTorch Module with an <code class="docutils literal"><span class="pre">act</span></code>
method, a <code class="docutils literal"><span class="pre">pi</span></code> module, and a <code class="docutils literal"><span class="pre">q</span></code> module. The <code class="docutils literal"><span class="pre">act</span></code> method and
<code class="docutils literal"><span class="pre">pi</span></code> module should accept batches of observations as inputs,
and <code class="docutils literal"><span class="pre">q</span></code> should accept a batch of observations and a batch of
actions as inputs. When called, these should return:</p>
<div class="wy-table-responsive"><table border="1" class="docutils">
<colgroup>
<col width="16%">
<col width="24%">
<col width="60%">
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Call</th>
<th class="head">Output Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">act</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Numpy array of actions for each</div>
<div class="line">observation.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Tensor containing actions from policy</div>
<div class="line">given observations.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Tensor containing the current estimate</div>
<div class="line">of Q* for the provided observations</div>
<div class="line">and actions. (Critical: make sure to</div>
<div class="line">flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table></div>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) – Any kwargs appropriate for the ActorCritic object
you provided to DDPG.</li>
<li><strong>seed</strong> (<em>int</em>) – Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) – Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) – Number of epochs to run and train agent.</li>
<li><strong>replay_size</strong> (<em>int</em>) – Maximum length of replay buffer.</li>
<li><strong>gamma</strong> (<em>float</em>) – Discount factor. (Always between 0 and 1.)</li>
<li><strong>polyak</strong> (<em>float</em>) – <p>Interpolation factor in polyak averaging for target
networks. Target networks are updated towards main networks
according to:</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg" alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta"></p>
</div><p>where <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg" alt="\rho"> is polyak. (Always between 0 and 1, usually
close to 1.)</p>
</li>
<li><strong>pi_lr</strong> (<em>float</em>) – Learning rate for policy.</li>
<li><strong>q_lr</strong> (<em>float</em>) – Learning rate for Q-networks.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Minibatch size for SGD.</li>
<li><strong>start_steps</strong> (<em>int</em>) – Number of steps for uniform-random action selection,
before running real policy. Helps exploration.</li>
<li><strong>update_after</strong> (<em>int</em>) – Number of env interactions to collect before
starting to do gradient descent updates. Ensures replay buffer
is full enough for useful updates.</li>
<li><strong>update_every</strong> (<em>int</em>) – Number of env interactions that should elapse
between gradient descent updates. Note: Regardless of how long
you wait between updates, the ratio of env steps to gradient steps
is locked to 1.</li>
<li><strong>act_noise</strong> (<em>float</em>) – Stddev for Gaussian exploration noise added to
policy at training time. (At test time, no noise is added.)</li>
<li><strong>num_test_episodes</strong> (<em>int</em>) – Number of episodes to test the deterministic
policy at the end of each epoch.</li>
<li><strong>max_ep_len</strong> (<em>int</em>) – Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) – Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) – How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="saved-model-contents-pytorch-version">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id11">Saved Model Contents: PyTorch Version</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#saved-model-contents-pytorch-version" title="Permalink to this headline">¶</a></h3>
<p>The PyTorch saved model can be loaded with <code class="docutils literal"><span class="pre">ac</span> <span class="pre">=</span> <span class="pre">torch.load('path/to/model.pt')</span></code>, yielding an actor-critic object (<code class="docutils literal"><span class="pre">ac</span></code>) that has the properties described in the docstring for <code class="docutils literal"><span class="pre">ddpg_pytorch</span></code>.</p>
<p>You can get actions from this model with</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">actions</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="documentation-tensorflow-version">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id12">Documentation: Tensorflow Version</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#documentation-tensorflow-version" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="spinup.ddpg_tf1">
<code class="descclassname">spinup.</code><code class="descname">ddpg_tf1</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=100</em>, <em>replay_size=1000000</em>, <em>gamma=0.99</em>, <em>polyak=0.995</em>, <em>pi_lr=0.001</em>, <em>q_lr=0.001</em>, <em>batch_size=100</em>, <em>start_steps=10000</em>, <em>update_after=1000</em>, <em>update_every=50</em>, <em>act_noise=0.1</em>, <em>num_test_episodes=10</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=1</em><span class="sig-paren">)</span><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#spinup.ddpg_tf1" title="Permalink to this definition">¶</a></dt>
<dd><p>Deep Deterministic Policy Gradient (DDPG)</p>
<table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name">
<col class="field-body">
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> – A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> – <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent’s Tensorflow computation graph:</p>
<div class="wy-table-responsive"><table border="1" class="docutils">
<colgroup>
<col width="17%">
<col width="25%">
<col width="58%">
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Deterministically computes actions</div>
<div class="line">from policy given states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the current estimate of Q* for</div>
<div class="line">states in <code class="docutils literal"><span class="pre">x_ph</span></code> and actions in</div>
<div class="line"><code class="docutils literal"><span class="pre">a_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the composition of <code class="docutils literal"><span class="pre">q</span></code> and</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code> for states in <code class="docutils literal"><span class="pre">x_ph</span></code>:</div>
<div class="line">q(x, pi(x)).</div>
</div>
</td>
</tr>
</tbody>
</table></div>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) – Any kwargs appropriate for the actor_critic
function you provided to DDPG.</li>
<li><strong>seed</strong> (<em>int</em>) – Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) – Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) – Number of epochs to run and train agent.</li>
<li><strong>replay_size</strong> (<em>int</em>) – Maximum length of replay buffer.</li>
<li><strong>gamma</strong> (<em>float</em>) – Discount factor. (Always between 0 and 1.)</li>
<li><strong>polyak</strong> (<em>float</em>) – <p>Interpolation factor in polyak averaging for target
networks. Target networks are updated towards main networks
according to:</p>
<div class="math">
<p><img src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/ea6c5fdb8bb78fe30797537bbb28553b9a7706ef.svg" alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta"></p>
</div><p>where <img class="math" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/b41ecbab285e58fd94a9b544487b74b1d992b0dd.svg" alt="\rho"> is polyak. (Always between 0 and 1, usually
close to 1.)</p>
</li>
<li><strong>pi_lr</strong> (<em>float</em>) – Learning rate for policy.</li>
<li><strong>q_lr</strong> (<em>float</em>) – Learning rate for Q-networks.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Minibatch size for SGD.</li>
<li><strong>start_steps</strong> (<em>int</em>) – Number of steps for uniform-random action selection,
before running real policy. Helps exploration.</li>
<li><strong>update_after</strong> (<em>int</em>) – Number of env interactions to collect before
starting to do gradient descent updates. Ensures replay buffer
is full enough for useful updates.</li>
<li><strong>update_every</strong> (<em>int</em>) – Number of env interactions that should elapse
between gradient descent updates. Note: Regardless of how long
you wait between updates, the ratio of env steps to gradient steps
is locked to 1.</li>
<li><strong>act_noise</strong> (<em>float</em>) – Stddev for Gaussian exploration noise added to
policy at training time. (At test time, no noise is added.)</li>
<li><strong>num_test_episodes</strong> (<em>int</em>) – Number of episodes to test the deterministic
policy at the end of each epoch.</li>
<li><strong>max_ep_len</strong> (<em>int</em>) – Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) – Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) – How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="saved-model-contents-tensorflow-version">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id13">Saved Model Contents: Tensorflow Version</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#saved-model-contents-tensorflow-version" title="Permalink to this headline">¶</a></h3>
<p>The computation graph saved by the logger includes:</p>
<div class="wy-table-responsive"><table border="1" class="docutils">
<colgroup>
<col width="10%">
<col width="90%">
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">a</span></code></td>
<td>Tensorflow placeholder for action input.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td><div class="first last line-block">
<div class="line">Deterministically computes an action from the agent, conditioned</div>
<div class="line">on states in <code class="docutils literal"><span class="pre">x</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q</span></code></td>
<td>Gives action-value estimate for states in <code class="docutils literal"><span class="pre">x</span></code> and actions in <code class="docutils literal"><span class="pre">a</span></code>.</td>
</tr>
</tbody>
</table></div>
<p>This saved model can be accessed either by</p>
<ul class="simple">
<li>running the trained policy with the <a class="reference external" href="https://spinningup.openai.com/en/latest/user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> tool,</li>
<li>or loading the whole saved graph into a program with <a class="reference external" href="https://spinningup.openai.com/en/latest/utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.</li>
</ul>
</div>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id14">References</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#references" title="Permalink to this headline">¶</a></h2>
<div class="section" id="relevant-papers">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id15">Relevant Papers</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#relevant-papers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic Policy Gradient Algorithms</a>, Silver et al. 2014</li>
<li><a class="reference external" href="https://arxiv.org/abs/1509.02971">Continuous Control With Deep Reinforcement Learning</a>, Lillicrap et al. 2016</li>
</ul>
</div>
<div class="section" id="why-these-papers">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id16">Why These Papers?</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#why-these-papers" title="Permalink to this headline">¶</a></h3>
<p>Silver 2014 is included because it establishes the theory underlying deterministic policy gradients (DPG). Lillicrap 2016 is included because it adapts the theoretically-grounded DPG algorithm to the deep RL setting, giving DDPG.</p>
</div>
<div class="section" id="other-public-implementations">
<h3><a class="toc-backref" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id17">Other Public Implementations</a><a class="headerlink" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#other-public-implementations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/openai/baselines/tree/master/baselines/ddpg">Baselines</a></li>
<li><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/ddpg.py">rllab</a></li>
<li><a class="reference external" href="https://github.com/ray-project/ray/tree/master/python/ray/rllib/agents/ddpg">rllib (Ray)</a></li>
<li><a class="reference external" href="https://github.com/sfujim/TD3">TD3 release repo</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="https://spinningup.openai.com/en/latest/algorithms/td3.html" class="btn btn-neutral float-right" title="Twin Delayed DDPG" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html" class="btn btn-neutral" title="Proximal Policy Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr>

  <div role="contentinfo">
    <p>
        © Copyright 2018, OpenAI.
      
        <span class="commit">
          Revision <code>038665d6</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org/">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions"><!-- Inserted RTD Footer -->

<div class="injected">

  

      
      
      
      <dl>
        <dt>Versionen</dt>
        
        <dd class="rtd-current-item">
          <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">latest</a>
        </dd>
        
      </dl>
      
      

      
      
      <dl>
        <dt>Downloads</dt>
        
        <dd><a href="https://spinningup.openai.com/_/downloads/en/latest/pdf/">PDF</a></dd>
        
        <dd><a href="https://spinningup.openai.com/_/downloads/en/latest/htmlzip/">HTML</a></dd>
        
        <dd><a href="https://spinningup.openai.com/_/downloads/en/latest/epub/">Epub</a></dd>
        
      </dl>
      
      

      

  
      <dl>
        
        <!-- These are kept as relative links for internal installs that are http -->
        <dt>On Read the Docs</dt>
        <dd>
          <a href="https://readthedocs.com/projects/openai-education-spinningup/">Project Home</a>
        </dd>
        <dd>
          <a href="https://readthedocs.com/projects/openai-education-spinningup/builds/">Erstellungsprozesse</a>
        </dd>
        <dd>
          <a href="https://readthedocs.com/projects/openai-education-spinningup/downloads/">Downloads</a>
        </dd>
      </dl>
      



      

      
      <dl>
        <dt>On GitHub</dt>
        <dd>
          <a href="https://github.com/openai/spinningup/blob/master/docs/algorithms/ddpg.rst">Ansehen</a>
        </dd>
        
        <dd>
          <a href="https://github.com/openai/spinningup/edit/master/docs/algorithms/ddpg.rst">Bearbeiten</a>
        </dd>
        
      </dl>
      
      

      

      <hr>

      
<div>
  
  <div>
    Documentation hosted by <a href="https://readthedocs.com/">Read the Docs</a>
  </div>
</div>


      

</div>
</div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/jquery.js.Download"></script>
      <script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/underscore.js.Download"></script>
      <script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/doctools.js.Download"></script>
      <script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/readthedocs-doc-embed.js.Download"></script>

  

  <script type="text/javascript" src="./Deep Deterministic Policy Gradient — Spinning Up documentation_files/theme.js.Download"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 


</body></html>